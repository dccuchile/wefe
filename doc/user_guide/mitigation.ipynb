{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(bias mitigation)=\n",
    "\n",
    "# Bias Mitigation (Debias)\n",
    "\n",
    "The following guide is designed to present the more general details on \n",
    "using the package to mitigate (debias) bias in word embedding models. \n",
    "The following sections show:\n",
    "\n",
    "- run {class}`~wefe.debias.hard_debias.HardDebias` mitigation method on an\n",
    "  embedding model to mitigate gender bias (using the ``fit-transform`` interface).\n",
    "- apply the ``target`` parameter when executing the transformation.\n",
    "- apply the ``ignore`` parameter when executing the transformation.\n",
    "- apply the ``copy`` parameter when executing the transformation.\n",
    "- run {class}`~wefe.debias.multiclass_hard_debias.MulticlassHardDebias` mitigation \n",
    "  method on an word embedding model to mitigate ethnic bias.\n",
    "\n",
    "## Hard Debias\n",
    "\n",
    "Hard debias is a method that allows mitigating biases through geometric operations on embeddings. \n",
    "This method is binary because it only allows 2 classes of the same bias criterion,\n",
    "such as male or female.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "For a multiclass debias (such as for Latinos, Asians and Whites), it is\n",
    "recommended to visit\n",
    "{class}`~wefe.debias.multiclass_hard_debias.MulticlassHardDebias` class.\n",
    "\n",
    ":::\n",
    "\n",
    "The main idea of this method is:\n",
    "\n",
    "1. Identify a bias subspace through the defining sets. In the case of gender,\n",
    "these could be e.g. ``[['woman', 'man'], ['she', 'he'], ...]``\n",
    "\n",
    "2. Neutralize the bias subspace of embeddings that should not be biased.\n",
    "First, it is defined a set of words that are correct to be related to the bias\n",
    "criterion: the *criterion specific gender words*.\n",
    "For example, in the case of gender, *gender specific* words are:\n",
    "``['he', 'his', 'He', 'her', 'she', 'him', 'him', 'She', 'man', 'women', 'men'...]``.\n",
    "\n",
    "Then, it is defined that all words outside this set should have no relation to the\n",
    "bias criterion and thus have the possibility of being biased. (e.g. for the case of\n",
    "genthe bias direction, such that neither is closer to the bias direction\n",
    "than the other: ``['doctor', 'nurse', ...]``). Therefore, this set of words is\n",
    "neutralized with respect to the bias subspace found in the previous step.\n",
    "\n",
    "The neutralization is carried out under the following operation:\n",
    "\n",
    "- $u$ : embedding\n",
    "- $v$ : bias direction\n",
    "\n",
    "First calculate the projection of the embedding on the bias subspace.\n",
    "\n",
    "\n",
    "$$\\text{bias\\_subspace} = \\frac{v \\cdot (v \\cdot u)}{(v \\cdot v)}$$\n",
    "\n",
    "Then subtract the projection from the embedding.\n",
    "\n",
    "$$u' = u - \\text{bias\\_subspace}$$\n",
    "\n",
    "  3. Equalizate the embeddings with respect to the bias direction.\n",
    "  Given an equalization set (set of word pairs such as ``['she', 'he'],\n",
    "  ['men', 'women'], ...``, but not limited to the definitional set) this step\n",
    "  executes, for each pair, an equalization with respect to the bias direction.\n",
    "  That is, it takes both embeddings of the pair and distributes them at the same\n",
    "  distance from the bias direction, so that neither is closer to the bias direction\n",
    "  than the other.\n",
    "\n",
    "\n",
    "The fit parameters define how the neutralization will be calculated. In\n",
    "Hard Debias, you have to provide the the ``definitional_pairs``, the\n",
    "``equalize_pairs`` (which could be the same of definitional pairs) and\n",
    "optionally, a debias ``criterion_name`` (to name the debiased model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wefe.word_embedding_model.WordEmbeddingModel at 0x7f98ec3eb1f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.utils import load_test_model\n",
    "\n",
    "model = load_test_model()  # load a reduced version of word2vec\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.datasets import fetch_debiaswe\n",
    "from wefe.debias.hard_debias import HardDebias\n",
    "\n",
    "debiaswe_wordsets = fetch_debiaswe()\n",
    "\n",
    "definitional_pairs = debiaswe_wordsets[\"definitional_pairs\"]\n",
    "equalize_pairs = debiaswe_wordsets[\"equalize_pairs\"]\n",
    "gender_specific = debiaswe_wordsets[\"gender_specific\"]\n",
    "\n",
    "hd = HardDebias(verbose=False, criterion_name=\"gender\").fit(\n",
    "    model, definitional_pairs=definitional_pairs, equalize_pairs=equalize_pairs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigation Parameters\n",
    "\n",
    "The parameters of the transform method are relatively standard for all\n",
    "methods. The most important ones are ``target``, ``ignore`` and\n",
    "``copy``.\n",
    "\n",
    "In the following example we use ``ignore`` and ``copy``, which are\n",
    "described below:\n",
    "\n",
    "-  ``ignore`` (by default, ``None``):\n",
    "\n",
    "    A list of strings that indicates that the debias method will perform\n",
    "    the debias in all words except those specified in this list. In case\n",
    "    it is not specified, debias will be executed on all words. In case\n",
    "    ignore is not specified or its value is None, the transformation will\n",
    "    be performed on all embeddings. This may cause words that are\n",
    "    specific to social groups to lose that component (for example,\n",
    "    leaving ``'she'`` and ``'he'`` without a gender component).\n",
    "\n",
    "-  ``copy`` (by default ``True``):\n",
    "\n",
    "    if the value of copy is ``True``, method attempts to create a copy of\n",
    "    the model and run debias on the copy. If ``False``, the method is\n",
    "    applied on the original model, causing the vectors to mutate.\n",
    "\n",
    "    **WARNING:** Setting copy with ``True`` requires at least 2x RAM of\n",
    "    the size of the model. Otherwise the execution of the debias may raise\n",
    "    ``MemoryError``.\n",
    "\n",
    "The following transformation is executed using a copy of the model,\n",
    "ignoring the words contained in ``gender_specific``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy argument is True. Transform will attempt to create a copy of the original model. This may fail due to lack of memory.\n",
      "Model copy created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13013/13013 [00:00<00:00, 105728.79it/s]\n"
     ]
    }
   ],
   "source": [
    "gender_debiased_model = hd.transform(model, ignore=gender_specific, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Decrease of Bias\n",
    "\n",
    "Using the metrics displayed in the {ref}`bias measurement` user guide, we\n",
    "can measure whether or not there was a change in the measured gender bias\n",
    "between the original model and the debiased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.datasets import load_weat\n",
    "from wefe.query import Query\n",
    "from wefe.metrics import WEAT\n",
    "\n",
    "weat_wordset = load_weat()\n",
    "weat = WEAT()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we measure the gender bias exposed by query 1 (Male terms and Female terms wrt Career and Family) with respect to the debiased model and the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Query: Male terms and Female terms wrt Career and Family\n",
      "- Target sets: [['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son'], ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']]\n",
      "- Attribute sets:[['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career'], ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']]> \n",
      " ---------------------------------------------------------------------- \n",
      "\n",
      "Debiased vs Biased (absolute values)\n",
      "0.047 < 0.463\n"
     ]
    }
   ],
   "source": [
    "gender_query_1 = Query(\n",
    "    [weat_wordset[\"male_terms\"], weat_wordset[\"female_terms\"]],\n",
    "    [weat_wordset[\"career\"], weat_wordset[\"family\"]],\n",
    "    [\"Male terms\", \"Female terms\"],\n",
    "    [\"Career\", \"Family\"],\n",
    ")\n",
    "print(gender_query_1, \"\\n\", \"-\" * 70, \"\\n\")\n",
    "\n",
    "biased_results_1 = weat.run_query(gender_query_1, model, normalize=True)\n",
    "debiased_results_1 = weat.run_query(\n",
    "    gender_query_1, gender_debiased_model, normalize=True\n",
    ")\n",
    "\n",
    "print(\"Debiased vs Biased (absolute values)\")\n",
    "print(\n",
    "    round(abs(debiased_results_1[\"weat\"]), 3),\n",
    "    \"<\",\n",
    "    round(abs(biased_results_1[\"weat\"]), 3),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show that there was a decrease in the measured gender bias.\n",
    "\n",
    "Next, we measure the gender bias exposed by query 2 (Male Names and Female Names wrt Pleasant and Unpleasant terms) with respect to the debiased model and the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Query: Male Names and Female Names wrt Pleasant and Unpleasant\n",
      "- Target sets: [['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill'], ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']]\n",
      "- Attribute sets:[['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation'], ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison']]> \n",
      " ---------------------------------------------------------------------- \n",
      "\n",
      "Debiased vs Biased (absolute values)\n",
      "0.055 < 0.074\n"
     ]
    }
   ],
   "source": [
    "gender_query_2 = Query(\n",
    "    [weat_wordset[\"male_names\"], weat_wordset[\"female_names\"]],\n",
    "    [weat_wordset[\"pleasant_5\"], weat_wordset[\"unpleasant_5\"]],\n",
    "    [\"Male Names\", \"Female Names\"],\n",
    "    [\"Pleasant\", \"Unpleasant\"],\n",
    ")\n",
    "\n",
    "print(gender_query_2, \"\\n\", \"-\" * 70, \"\\n\")\n",
    "\n",
    "biased_results_2 = weat.run_query(\n",
    "    gender_query_2, model, normalize=True, preprocessors=[{}, {\"lowercase\": True}]\n",
    ")\n",
    "debiased_results_2 = weat.run_query(\n",
    "    gender_query_2,\n",
    "    gender_debiased_model,\n",
    "    normalize=True,\n",
    "    preprocessors=[{}, {\"lowercase\": True}],\n",
    ")\n",
    "\n",
    "print(\"Debiased vs Biased (absolute values)\")\n",
    "print(\n",
    "    round(abs(debiased_results_2[\"weat\"]), 3),\n",
    "    \"<\",\n",
    "    round(abs(biased_results_2[\"weat\"]), 3),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the above results show that there was a decrease in the measured gender bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Parameter\n",
    "\n",
    "If a set of words is specified in ``target`` parameter, the debias method is performed\n",
    "only on the embeddings associated with this set. \n",
    "In the case of providing ``None``, the transformation is performed on all vocabulary\n",
    "words except those specified in ignore. By default ``None``.\n",
    "\n",
    "In the following example, the target parameter is used to execute the transformation \n",
    "only on the career and family word set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy argument is True. Transform will attempt to create a copy of the original model. This may fail due to lack of memory.\n",
      "Model copy created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 16710.37it/s]\n"
     ]
    }
   ],
   "source": [
    "targets = [\n",
    "    \"executive\",\n",
    "    \"management\",\n",
    "    \"professional\",\n",
    "    \"corporation\",\n",
    "    \"salary\",\n",
    "    \"office\",\n",
    "    \"business\",\n",
    "    \"career\",\n",
    "    \"home\",\n",
    "    \"parents\",\n",
    "    \"children\",\n",
    "    \"family\",\n",
    "    \"cousins\",\n",
    "    \"marriage\",\n",
    "    \"wedding\",\n",
    "    \"relatives\",\n",
    "]\n",
    "\n",
    "hd = HardDebias(verbose=False, criterion_name=\"gender\").fit(\n",
    "    model, definitional_pairs=definitional_pairs, equalize_pairs=equalize_pairs,\n",
    ")\n",
    "\n",
    "gender_debiased_model = hd.transform(model, target=targets, copy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a bias test is run on the mitigated embeddings associated with the\n",
    "target words. \n",
    "\n",
    "In this case, the value of the metric is lower on the\n",
    "query executed on the mitigated model than on the original one.\n",
    "These results indicate that there was a mitigation of bias on embeddings of these words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Query: Male terms and Female terms wrt Career and Family\n",
      "- Target sets: [['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son'], ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']]\n",
      "- Attribute sets:[['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career'], ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']]> \n",
      " ---------------------------------------------------------------------- \n",
      "\n",
      "Debiased vs Biased (absolute values)\n",
      "0.047 < 0.463\n"
     ]
    }
   ],
   "source": [
    "gender_query_1 = Query(\n",
    "    [weat_wordset[\"male_terms\"], weat_wordset[\"female_terms\"]],\n",
    "    [weat_wordset[\"career\"], weat_wordset[\"family\"]],\n",
    "    [\"Male terms\", \"Female terms\"],\n",
    "    [\"Career\", \"Family\"],\n",
    ")\n",
    "print(gender_query_1, \"\\n\", \"-\" * 70, \"\\n\")\n",
    "\n",
    "biased_results_1 = weat.run_query(gender_query_1, model, normalize=True)\n",
    "debiased_results_1 = weat.run_query(\n",
    "    gender_query_1, gender_debiased_model, normalize=True\n",
    ")\n",
    "\n",
    "print(\"Debiased vs Biased (absolute values)\")\n",
    "print(\n",
    "    round(abs(debiased_results_1[\"weat\"]), 3),\n",
    "    \"<\",\n",
    "    round(abs(biased_results_1[\"weat\"]), 3),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if a bias test is run with words that were outside the ``target``\n",
    "word set, the results are almost the same. The slight difference in the\n",
    "metric scores lies in the fact that the equalize sets were still\n",
    "equalized.\n",
    "\n",
    ":::{warning}\n",
    "\n",
    "The equalization process can modify embeddings that have not been marked in the target.\n",
    "\n",
    "Equalization can be deactivated by delivering an empty equalize set (``[]``)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Query: Male Names and Female Names wrt Pleasant and Unpleasant\n",
      "- Target sets: [['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill'], ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']]\n",
      "- Attribute sets:[['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation'], ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison']]> \n",
      " ---------------------------------------------------------------------- \n",
      "\n",
      "Debiased vs Biased (absolute values)\n",
      "0.08 > 0.074\n"
     ]
    }
   ],
   "source": [
    "gender_query_2 = Query(\n",
    "    [weat_wordset[\"male_names\"], weat_wordset[\"female_names\"]],\n",
    "    [weat_wordset[\"pleasant_5\"], weat_wordset[\"unpleasant_5\"]],\n",
    "    [\"Male Names\", \"Female Names\"],\n",
    "    [\"Pleasant\", \"Unpleasant\"],\n",
    ")\n",
    "\n",
    "print(gender_query_2, \"\\n\", \"-\" * 70, \"\\n\")\n",
    "\n",
    "biased_results_2 = weat.run_query(\n",
    "    gender_query_2, model, normalize=True, preprocessors=[{}, {\"lowercase\": True}]\n",
    ")\n",
    "debiased_results_2 = weat.run_query(\n",
    "    gender_query_2,\n",
    "    gender_debiased_model,\n",
    "    normalize=True,\n",
    "    preprocessors=[{}, {\"lowercase\": True}],\n",
    ")\n",
    "\n",
    "print(\"Debiased vs Biased (absolute values)\")\n",
    "print(\n",
    "    round(abs(debiased_results_2[\"weat\"]), 3),\n",
    "    \">\",\n",
    "    round(abs(biased_results_2[\"weat\"]), 3),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the equalization caused the bias of the debiased model to be slightly larger than the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Saving the Debiased Model\n",
    "\n",
    "To save the mitigated model one must access the ``KeyedVectors`` (the\n",
    "gensim object that contains the embeddings) through ``wv`` and then use\n",
    "the ``save`` method to store the method in a file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_debiased_model.wv.save(\"gender_debiased_glove.kv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Hard Debias\n",
    "\n",
    "Multiclass Hard Debias is a generalized version of Hard Debias that\n",
    "enables multiclass debiasing. Generalized refers to the fact that this\n",
    "method extends Hard Debias in order to support more than two types of\n",
    "social target sets within the definitional set.\n",
    "\n",
    "For example, for the case of religion bias, it supports a debias using\n",
    "words associated with Christianity, Islam and Judaism.\n",
    "\n",
    "The usage is very similar to Hard Debias with the difference that the\n",
    "``definitional_sets`` can be larger than pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity_definitional_sets: \n",
      "[['black', 'caucasian', 'asian'], ['african', 'caucasian', 'asian'], ['black', 'white', 'asian'], ['africa', 'america', 'asia'], ['africa', 'america', 'china'], ['africa', 'europe', 'asia']]\n",
      "ethnicity_equalize_sets: \n",
      "[['manager', 'executive', 'redneck', 'hillbilly', 'leader', 'farmer'], ['doctor', 'engineer', 'laborer', 'teacher'], ['slave', 'musician', 'runner', 'criminal', 'homeless']]\n",
      "---------------------------------------------------------------------- \n",
      "\n",
      "copy argument is True. Transform will attempt to create a copy of the original model. This may fail due to lack of memory.\n",
      "Model copy created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13003/13003 [00:00<00:00, 16302.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from wefe.datasets import fetch_debias_multiclass\n",
    "from wefe.debias.multiclass_hard_debias import MulticlassHardDebias\n",
    "\n",
    "multiclass_debias_wordsets = fetch_debias_multiclass()\n",
    "weat_wordsets = load_weat()\n",
    "weat = WEAT()\n",
    "\n",
    "ethnicity_definitional_sets = multiclass_debias_wordsets[\"ethnicity_definitional_sets\"]\n",
    "ethnicity_equalize_sets = list(\n",
    "    multiclass_debias_wordsets[\"ethnicity_analogy_templates\"].values()\n",
    ")\n",
    "\n",
    "print(f\"ethnicity_definitional_sets: \\n{ethnicity_definitional_sets}\")\n",
    "print(f\"ethnicity_equalize_sets: \\n{ethnicity_equalize_sets}\")\n",
    "print(\"-\" * 70, \"\\n\")\n",
    "\n",
    "mhd = MulticlassHardDebias(verbose=False, criterion_name=\"ethnicity\")\n",
    "mhd.fit(\n",
    "    model=model,\n",
    "    definitional_sets=ethnicity_definitional_sets,\n",
    "    equalize_sets=ethnicity_equalize_sets,\n",
    ")\n",
    "\n",
    "ethnicity_debiased_model = mhd.transform(model, copy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Decrease of Bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Query: european_american_names and african_american_names wrt white_biased_words and black_biased_words\n",
      "- Target sets: [['america', 'caucasian', 'europe', 'white'], ['africa', 'african', 'black']]\n",
      "- Attribute sets:[['manager', 'executive', 'redneck', 'hillbilly', 'leader', 'farmer'], ['slave', 'musician', 'runner', 'criminal', 'homeless']]> \n",
      " ---------------------------------------------------------------------- \n",
      "\n",
      "Debiased vs Biased (absolute values)\n",
      "0.08 < 0.074\n"
     ]
    }
   ],
   "source": [
    "ethnicity_query = Query(\n",
    "    [\n",
    "        multiclass_debias_wordsets[\"white_terms\"],\n",
    "        multiclass_debias_wordsets[\"black_terms\"],\n",
    "    ],\n",
    "    [\n",
    "        multiclass_debias_wordsets[\"white_biased_words\"],\n",
    "        multiclass_debias_wordsets[\"black_biased_words\"],\n",
    "    ],\n",
    "    [\"european_american_names\", \"african_american_names\"],\n",
    "    [\"white_biased_words\", \"black_biased_words\"],\n",
    ")\n",
    "\n",
    "print(ethnicity_query, \"\\n\", \"-\" * 70, \"\\n\")\n",
    "\n",
    "biased_results = weat.run_query(\n",
    "    ethnicity_query, model, normalize=True, preprocessors=[{}, {\"lowercase\": True}],\n",
    ")\n",
    "debiased_results = weat.run_query(\n",
    "    ethnicity_query,\n",
    "    ethnicity_debiased_model,\n",
    "    normalize=True,\n",
    "    preprocessors=[{}, {\"lowercase\": True}],\n",
    ")\n",
    "\n",
    "print(\"Debiased vs Biased (absolute values)\")\n",
    "print(\n",
    "    round(abs(debiased_results_2[\"weat\"]), 3),\n",
    "    \"<\",\n",
    "    round(abs(biased_results_2[\"weat\"]), 3),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37d01894bb315c73bf6fde5551d8a97078996f38b23395695bd1998fb0ae5507"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
