FastText is an open-source framework that enables users to learn text representations and classifiers. We use fastText to get our pre-trained word embeddings for 7
languages which are English, Dutch, Swedish, Italian, French, Spanish and German. All these were learned using Common Crawl and Wikipedia data. These models were 
created with Continuous Bag Of Words (CBOW) with position-weights in a dimensionality of 300, character n-grams of length 5, a window of size 5, and 10 negatives.
Here, CBOW is a neural network-based system for learning the essential representation of words for every word.

We have word sets defined from previous work (Badilla et al., 2020) that have been validated to examine bias from word embeddings. These embeddings have been used
originally in English only. In this research, word embeddings have to be used in 7 languages that are in focus to identify bias. We must also keep in mind that the
queries are only legitimate if the language of the word sets are same as the embeddings model’s language. Hence, a Google translator is used, similar to Kurpicz-
Briki and Leoni (2021), for this language translation to investigate bias in a particular language. Once this is complete, we have 7 different word sets for 7 
embedding models (1 for each language).

Let us understand this with the help of an example. If we consider a query of Male and Female terms with respect to Career and Family, the attribute set for 
‘career’ is initially defined as:

Career: [ “executive”, “management”, “professional”, “corporation”, “salary”, “office”, “business”, “career” ]

If the bias is to be measured with respect to Swedish, this attribute set must be translated to Swedish. This is done with the help of Google translator which will
translate to :

Career: [‘verkst allande’, ‘f orvaltning’, ‘professionell’,‘f oretag’, ‘l on’, ‘kontor’, ‘f oretag’, ‘karriar’].

Queries, fairness metrics and embeddings models are the essential components that are given as an input to generate a bias score. As discussed earlier, we will 
be working with 4 different fairness metrics (WEAT, WEAT-ES, RND and RNSB), 3 different set of queries and 7 different languages (word embeddings). Each time a 
model in a respective language is selected, we evaluate them with all the 4 fairness metrics for each of the query to generate a score. This is the bias score 
which will identify an embedding model as biased or not.

The following example demonstrates how to use any sample gender query on any word embedding model considering a fairness metric. The workflow may be broken 
down into three steps which are:

• Download and install the word embedding model in any desired language.
• Structure the query based on the target set and the attribute set for that particular language using google translator.
• Execute the query utilizing the fairness metric through the Word Embedding Model.

Any score of greater than 0 suggests that there is indeed a bias for the query in consideration for the particular language. 

In majority of the cases, a score will be achieved. Nevertheless, in a few occurrences, certain queries will contain about 20% missing words and the results will 
reflect that and the query declares itself invalid and returns NaN. In such cases, a “lost vocabulary threshold” parameter is introduced which can limit the number
of words lost or missing. A 30% loss is permitted in the following scenario:

BiasScore = WEAT().run_query(query, model,
            lost_vocabulary_threshold = 0.3,
            warn_not_found_words = True
