{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark - WEFE, Fair Embedding Engine and Responsibly.AI\n",
    "\n",
    "To the best of our knowledge, we are aware of only three Python libraries that implement bias measurement and mitigation methods: Fair Embedding Engine (FEE) and Responsibly.\n",
    "\n",
    "According to its authors, Fair Embedding Engine is defined as \"A Library for Analyzing and Mitigating Gender Bias in Word Embeddings\", while Responsibly is defined as \"Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems.\"\n",
    "\n",
    "The FEE and Responsibly documentation can be found at the following links respectively: \n",
    "- https://github.com/FEE-Fair-Embedding-Engine/FEE\n",
    "- https://docs.responsibly.ai/\n",
    "\n",
    "The following document shows a comparison in various areas between these libraries with respect to WEFE.\n",
    "\n",
    "The points to be evaluated are:\n",
    "    \n",
    "1. Ease of installation\n",
    "2. Quality of the package and documentation.\n",
    "3. Ease of loading models\n",
    "4. Ease of running bias measurements. \n",
    "5. Performance in execution times.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ease of installation\n",
    "\n",
    "This comparison aims to evaluate how easy it is to install the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "According to the documentation, WEFE is available for installation using the Python Package Index (via pip) as well as via conda.\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install --upgrade wefe\n",
    "# or\n",
    "conda install -c pbadilla wefe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair Embedding Engine\n",
    "\n",
    "In the case of FEE, neither the documentation nor the repository indicates how to install the package. Therefore, the easiest thing to do in this case is to clone the repository and then install the requirements manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clone the repo\n",
    "```bash\n",
    "$ git clone https://github.com/FEE-Fair-Embedding-Engine/FEE\n",
    "```\n",
    "\n",
    "2. Install the requirements.\n",
    "```bash\n",
    "$ pip install -r FEE/requirements.txt\n",
    "$ pip install sympy\n",
    "$ pip install -U gensim==3.8.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly\n",
    "\n",
    "According to its documentation, responsibly is also hosted in the Python Package Index so it can be installed using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pip install responsibly\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBiasScores\n",
    "\n",
    "In the case of EmbeddingBiasScores, the documentation indicates that the repository can be cloned and then installed locally."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ git clone https://github.com/HammerLabML/EmbeddingBiasScores.git\n",
    "$ pip install -r EmbeddingBiasScores/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Both WEFE and responsibly are easy to install, which lowers the initial barriers to entry. FEE and EmbeddingBiasScores requires more knowledge of Python and Pip to be able to use it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Source code quality and documentation\n",
    "\n",
    "This benchmark seeks to compare the quality of the documentation as well as the quality and best practices of the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "WEFE has a complete documentation page, which explains in detail the use of the package: an about with the motivation and objectives of the project, quick start showing how to install the library, multiple user manuals to measure and mitigate bias, detailed API of the implemented methods, theoretical manuals and finally implementations of previous case studies.\n",
    "\n",
    "In addition, most of the code is tested and was developed using continuous integration mechanisms (through a linter and testing mechanisms in Github Actions) that ensures good code quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair Embedding Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEE has a documentation, which covers only the basic aspects of the API plus a flowchart showing the main concepts of the library.\n",
    "The documentation does not contain user guides, code examples or theoretical information about the implemented methods.\n",
    "\n",
    "No tests, linters or continuous code integration mechanisms could be identified, which makes the code prone to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responsibly has also a complete documentation page, which explains the use of the package: an index with the main project information and a quick start showing how to install the library, demos that act as user manuals, and a detailed API of the implemented methods.\n",
    "\n",
    "In addition, most of the code is tested and was developed using continuous integration mechanisms (through a linter and testing in Github Actions) that keep it with a good code quality.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBiasScores\n",
    "\n",
    "It was not possible to find formal documentation explaining how to run bias tests in EmbeddingBiasScores.\n",
    "There is only a small notebook with some use cases, which at the time of creating this document, had several flaws that made it difficult to understand and use.\n",
    "\n",
    "No tests, linters or continuous code integration mechanisms could be identified, which makes the code prone to errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In terms of documentation, WEFE contains much more detailed documentation than the other libraries with more extensive manuals and replications of previous case studies. \n",
    "Responsibly has sufficient documentation to execute its main functionalities without major problems, however, it is not exhaustive.\n",
    "FEE, only has API documentation, which makes it insufficient for new users to use it directly.\n",
    "Lastly, EmbeddingBiasScores only presents a notebook with examples.\n",
    "\n",
    "\n",
    "With respect to software quality, both WEFE and Responsibly comply with best practices. \n",
    "FEE and EmbeddingBiasScores lacks testing and code quality control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ease of loading models\n",
    "\n",
    "This comparison looks at how easy it is to load a Word Embedding model.\n",
    "In this benchmark, two tests will be compared: loading a model from gensim API (`glove-twitter-25`) and loading a model from a binary file (`word2vec`).\n",
    "\n",
    "For the second test, you need to download the original word2vec model, which can be downloaded using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/word2vec-google-news-300.gz\n",
    "!gzip -dv word2vec-google-news-300.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "In WEFE, models are simply wrappers of Gensim models. This implies that the model reading process (either loaded by the API or by a file) is handled by the Gensim loaders, while the class that generates the objects that allow access to the embeddings is managed by WEFE.\n",
    "\n",
    "The following code can be used for the loading of the glove model from the gensim API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "import gensim.downloader as api\n",
    "\n",
    "# load glove\n",
    "twitter_25 = api.load(\"glove-twitter-25\")\n",
    "model = WordEmbeddingModel(twitter_25, \"glove twitter dim=25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows you to load word2vec from its original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# load word2vec\n",
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "# word2vec = KeyedVectors.load_word2vec_format('word2vec-google-news-300', binary=True)\n",
    "model = WordEmbeddingModel(word2vec, \"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEE\n",
    "\n",
    "FEE also offers direct support for loading models from the FEE API through the following code.\n",
    "In this case, the model loading is coupled to the class which then has the methods to access the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FEE.fee.embedding.loader import WE\n",
    "\n",
    "fee_model = WE().load(ename=\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from FEE.fee.embedding.loader import WE\n",
    "\n",
    "fee_model = WE().load(fname=\"word2vec-google-news-300\", format=\"bin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly and EmbeddingBiasScores\n",
    "\n",
    "Neither Responsibly nor EmbeddingBiasScores implement intermediate interfaces to handle embedding models, they simply use the gensim or similar interfaces for this purpose. The above can be reflected in the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_589/2998407189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter_25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove-twitter-25'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "# load twitter_25 model from gensim api\n",
    "twitter_25 = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# load word2vec model from file\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"word2vec-google-news-300\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both WEFE and FEE implement interfaces to internally manage the use of embedding models according to their needs.\n",
    "Responsibly and EmbeddingBiasScores do not implement wrappers, which may make it difficult to use them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ease of running bias measurements. \n",
    "\n",
    "This benchmark is intended to show how easy it is to run queries on the metrics that can be used. \n",
    "To keep the comparison simple, the set of words and the embeddings model will be kept fixed; only the metrics executed will be varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words to evaluate\n",
    "\n",
    "female_terms = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
    "male_terms = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "\n",
    "family_terms = [\n",
    "    \"home\",\n",
    "    \"parents\",\n",
    "    \"children\",\n",
    "    \"family\",\n",
    "    \"cousins\",\n",
    "    \"marriage\",\n",
    "    \"wedding\",\n",
    "    \"relatives\",\n",
    "]\n",
    "career_terms = [\n",
    "    \"executive\",\n",
    "    \"management\",\n",
    "    \"professional\",\n",
    "    \"corporation\",\n",
    "    \"salary\",\n",
    "    \"office\",\n",
    "    \"business\",\n",
    "    \"career\",\n",
    "]\n",
    "\n",
    "# optional, only for wefe usage.\n",
    "target_sets_names = [\"Female terms\", \"Male terms\"]\n",
    "attribute_sets_names = [\"Family terms\", \"Career terms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "WEFE defines a standardized framework to execute metrics: in short, it is necessary to define a query that will act as a container for the words to be tested and then, together with the model, be delivered as input to some metric.\n",
    "\n",
    "The outputs of the metrics are always dictionaries since most of them contain additional information that could eventually be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Query: Female ferms and Male terms wrt Family terms and Career terms\n",
       "- Target sets: [['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter'], ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']]\n",
       "- Attribute sets:[['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives'], ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']]>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the modules\n",
    "from wefe.query import Query\n",
    "\n",
    "# 1. create the query\n",
    "query = Query(\n",
    "    [female_terms, male_terms],\n",
    "    [family_terms, career_terms],\n",
    "    target_sets_names,\n",
    "    attribute_sets_names,\n",
    ")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female ferms and Male terms wrt Family terms and Career terms',\n",
       " 'result': 0.31658415612764657,\n",
       " 'weat': 0.31658415612764657,\n",
       " 'effect_size': 0.677943967611404,\n",
       " 'p_value': nan}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.metrics.WEAT import WEAT\n",
    "\n",
    "# 2. instance a WEAT metric and pass the query plus the model.\n",
    "weat = WEAT()\n",
    "result = weat.run_query(query, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As run query is independent of the query and the model, it can take several parameters that customize the performance of the metric. In this case, we show how to standardize the words before searching for them in the model by making them all lowercase and then removing their accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female ferms and Male terms wrt Family terms and Career terms',\n",
       " 'result': 0.31658415612764657,\n",
       " 'weat': 0.31658415612764657,\n",
       " 'effect_size': 0.677943967611404,\n",
       " 'p_value': nan}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weat = WEAT()\n",
    "result = weat.run_query(\n",
    "    query,\n",
    "    model,\n",
    "    preprocessors=[{\"lowercase\": True, \"strip_accents\": True}],\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we show how to calculate the p-value through a permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female ferms and Male terms wrt Family terms and Career terms',\n",
       " 'result': 0.31658415612764657,\n",
       " 'weat': 0.31658415612764657,\n",
       " 'effect_size': 0.677943967611404,\n",
       " 'p_value': 0.08699130086991301}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weat = WEAT()\n",
    "result = weat.run_query(\n",
    "    query,\n",
    "    model,\n",
    "    calculate_p_value=True,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interface makes it possible for us to switch very easily to similar metrics (i.e. supporting the same number of word sets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female ferms and Male terms wrt Family terms and Career terms',\n",
       " 'result': 0.2316374702204208,\n",
       " 'rnsb': 0.2316374702204208,\n",
       " 'negative_sentiment_probabilities': {'female': 0.13601304241096868,\n",
       "  'woman': 0.09092891800011083,\n",
       "  'girl': 0.018634003460932136,\n",
       "  'sister': 0.02660626015429457,\n",
       "  'she': 0.024974915876983528,\n",
       "  'her': 0.012636440088022338,\n",
       "  'hers': 0.18909707392930308,\n",
       "  'daughter': 0.02508881957615572,\n",
       "  'male': 0.08005167123803347,\n",
       "  'man': 0.06748459956816788,\n",
       "  'boy': 0.04567972254971964,\n",
       "  'brother': 0.05599846481670445,\n",
       "  'he': 0.06136734334386551,\n",
       "  'him': 0.03019750326319992,\n",
       "  'his': 0.05847721064473965,\n",
       "  'son': 0.08768651868532318},\n",
       " 'negative_sentiment_distribution': {'female': 0.13454349011675998,\n",
       "  'woman': 0.08994647692175288,\n",
       "  'girl': 0.018432672455824813,\n",
       "  'sister': 0.02631879293823218,\n",
       "  'she': 0.024705074512698814,\n",
       "  'her': 0.012499909728927259,\n",
       "  'hers': 0.18705397545951583,\n",
       "  'daughter': 0.02481774753987463,\n",
       "  'male': 0.07918675332253212,\n",
       "  'man': 0.06675546252100513,\n",
       "  'boy': 0.045186176196503565,\n",
       "  'brother': 0.05539342965989278,\n",
       "  'he': 0.06070430016358005,\n",
       "  'him': 0.029871234477404193,\n",
       "  'his': 0.05784539388997399,\n",
       "  'son': 0.08673911009552165}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.metrics import RNSB\n",
    "\n",
    "rnsb = RNSB()\n",
    "result = rnsb.run_query(query, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female ferms and Male terms wrt Family terms and Career terms',\n",
       " 'result': 0.4357533518195851,\n",
       " 'mac': 0.4357533518195851,\n",
       " 'targets_eval': {'Female ferms': {'female': {'Family terms': 0.31804752349853516,\n",
       "    'Career terms': 0.43660861998796463},\n",
       "   'woman': {'Family terms': 0.24169018119573593,\n",
       "    'Career terms': 0.3958834111690521},\n",
       "   'girl': {'Family terms': 0.27893901616334915,\n",
       "    'Career terms': 0.5540130585432053},\n",
       "   'sister': {'Family terms': 0.26786402612924576,\n",
       "    'Career terms': 0.5402307193726301},\n",
       "   'she': {'Family terms': 0.3126588687300682,\n",
       "    'Career terms': 0.5178160294890404},\n",
       "   'her': {'Family terms': 0.31602276116609573,\n",
       "    'Career terms': 0.5942907352000475},\n",
       "   'hers': {'Family terms': 0.4396950639784336,\n",
       "    'Career terms': 0.5640630088746548},\n",
       "   'daughter': {'Family terms': 0.2509744167327881,\n",
       "    'Career terms': 0.5034244172275066}},\n",
       "  'Male terms': {'male': {'Family terms': 0.3604205995798111,\n",
       "    'Career terms': 0.5834408048540354},\n",
       "   'man': {'Family terms': 0.3802716135978699,\n",
       "    'Career terms': 0.5215189531445503},\n",
       "   'boy': {'Family terms': 0.32475025951862335,\n",
       "    'Career terms': 0.5474967788904905},\n",
       "   'brother': {'Family terms': 0.281925693154335,\n",
       "    'Career terms': 0.48872119933366776},\n",
       "   'he': {'Family terms': 0.400515902787447,\n",
       "    'Career terms': 0.5290917791426182},\n",
       "   'him': {'Family terms': 0.29920244961977005,\n",
       "    'Career terms': 0.47434185072779655},\n",
       "   'his': {'Family terms': 0.300054918974638,\n",
       "    'Career terms': 0.39712197706103325},\n",
       "   'son': {'Family terms': 0.6768742799758911,\n",
       "    'Career terms': 0.846136340405792}}}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.metrics import MAC\n",
    "\n",
    "mac = MAC()\n",
    "result = mac.run_query(query, model)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fair Embedding Engine\n",
    "\n",
    "In the case of Fair Embedding Engine, the embedding model is passed in the metric instantation.\n",
    "Then, the metric value is calculated using the compute method of the metric object.\n",
    "\n",
    "FEE differs somewhat from WEFE normalization by making each instance of the metric model-dependent.\n",
    "On the other hand, it is not clear how to pass different size of word sets to the compute method: the word sets are delivered directly as star * parameter, which represents an arbitrary number of positional arguments.\n",
    "This lack of definition makes it difficult to understand how many and which word sets to pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FEE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_709/4076166106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mFEE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWEAT\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFEE_WEAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfee_weat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFEE_WEAT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfee_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfee_weat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfemale_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmale_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcareer_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FEE'"
     ]
    }
   ],
   "source": [
    "from FEE.fee.metrics import WEAT as FEE_WEAT\n",
    "\n",
    "fee_weat = FEE_WEAT(fee_model)\n",
    "\n",
    "fee_weat.compute(female_terms, male_terms, family_terms, career_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEAT's implementation of FEE also allows the p_value to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7264477, 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fee_weat.compute(female_terms, male_terms, family_terms, career_terms, p_val=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the metric does not contain the possibility of executing more complex actions such as preprocessing word sets.\n",
    "\n",
    "We were not able to find any other metric that was easily replaceable using the same or similar interface (with respect to the WEFE standardization layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly\n",
    "\n",
    "Similar to WEFE, responsibly has a function that receives the model and word sets as inputs and responds the value of weat as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Target words': 'female_terms vs. male_terms',\n",
       " 'Attrib. words': 'family_terms vs. career_terms',\n",
       " 's': 0.31658387184143066,\n",
       " 'd': 0.6779436,\n",
       " 'p': 0.09673659673659674,\n",
       " 'Nt': '8x2',\n",
       " 'Na': '8x2'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from responsibly.we.weat import calc_single_weat\n",
    "\n",
    "calc_single_weat(\n",
    "    twitter_25,\n",
    "    first_target={\"name\": \"female_terms\", \"words\": female_terms},\n",
    "    second_target={\"name\": \"male_terms\", \"words\": male_terms},\n",
    "    first_attribute={\"name\": \"family_terms\", \"words\": family_terms},\n",
    "    second_attribute={\"name\": \"career_terms\", \"words\": career_terms},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same function can be used to calculate weat with the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Target words': 'female_terms vs. male_terms',\n",
       " 'Attrib. words': 'family_terms vs. career_terms',\n",
       " 's': 0.31658387184143066,\n",
       " 'd': 0.6779436,\n",
       " 'p': None,\n",
       " 'Nt': '8x2',\n",
       " 'Na': '8x2'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_single_weat(\n",
    "    twitter_25,\n",
    "    first_target={\"name\": \"female_terms\", \"words\": female_terms},\n",
    "    second_target={\"name\": \"male_terms\", \"words\": male_terms},\n",
    "    first_attribute={\"name\": \"family_terms\", \"words\": family_terms},\n",
    "    second_attribute={\"name\": \"career_terms\", \"words\": career_terms},\n",
    "    with_pvalue=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the metric does not contain the possibility of executing more complex actions such as preprocessing word sets.\n",
    "\n",
    "We were unable to find other metrics directly comparable to those implemented by WEFE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBiasScores\n",
    "\n",
    "EmbeddingBiasScores formalizes how bias is measured in a different way than WEFE: it classifies the methods into clustering or geometric methods (note that WEFE only implements the geometric equivalents).\n",
    "\n",
    "Under their standardization, each geometric metric must first define the direction of the bias using `define_bias_space` on the `attribute_embeddings`; and then use the `group_bias` or `mean_individual_bias` methods to calculate the value of the metric.\n",
    "\n",
    "Examples of usage are shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embeddings to be used must be transformed by hand from words to arrays.\n",
    "target_embeddings = [\n",
    "    [model[word] for word in female_terms],\n",
    "    [model[word] for word in male_terms],\n",
    "]\n",
    "attribute_embeddings = [\n",
    "    [model[word] for word in family_terms],\n",
    "    [model[word] for word in career_terms],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6564166411275919"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from EmbeddingBiasScores.geometrical_bias import WEAT\n",
    "\n",
    "weat = WEAT()\n",
    "weat.define_bias_space(attribute_embeddings)\n",
    "# group bias returns the effect size.\n",
    "weat.group_bias(target_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `WEAT` return the effect size. There is no way to parametrize the metric to calculate weat score or the p-value.\n",
    "\n",
    "Similar to WEFE, the standarization implemented by EmbeddingBiasScores allows to easily change the use metric to some other with the same input word sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4357533518195851"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from EmbeddingBiasScores.geometrical_bias import MAC\n",
    "\n",
    "mac = MAC()\n",
    "mac.define_bias_space(attribute_embeddings)\n",
    "\n",
    "# mac does not accept more than one target set, so we have to calculate it manually.\n",
    "target_0_mac = mac.mean_individual_bias(target_embeddings[0])\n",
    "target_1_mac = mac.mean_individual_bias(target_embeddings[1])\n",
    "(target_0_mac + target_1_mac) / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EmbeddingBiasScores includes metrics that WEFE does not implement yet, such as GeneralizedWEAT and SAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019786509"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from EmbeddingBiasScores.geometrical_bias import GeneralizedWEAT\n",
    "\n",
    "gweat = GeneralizedWEAT()\n",
    "gweat.define_bias_space(attribute_embeddings)\n",
    "gweat.group_bias(target_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31590998622684796"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from EmbeddingBiasScores.geometrical_bias import SAME\n",
    "\n",
    "same = SAME()\n",
    "same.define_bias_space(attribute_embeddings)\n",
    "same.mean_individual_bias(target_embeddings[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, no metric implements the possibility of of executing more complex actions such as preprocessing word sets or customize some execution settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In WEFE, the use of metric decoupled queries allows both parameterization of metric execution as well as the easy interchange of one metric for another.\n",
    "Furthermore, the clean and unified interface for all metrics makes it intuitive how to run bias measurements.\n",
    "\n",
    "Both responsibly and FEE have similar interfaces, where the metric arguments are sets of words (and not queries), making it difficult to standardize inputs across metrics.\n",
    "We were unable to find any metric other than WEAT to include in the benchmarking on FEE and liability.\n",
    "\n",
    "\n",
    "On the other hand, EmbeddingBiasScores also presents its own mathematical standardization for each metric as well as some metrics that wefe does not implement. \n",
    "While the standardization they present may be a bit more specific, it makes it more complex to use. \n",
    "\n",
    "The increased difficulty is mainly due to two factors: first, you have to manually define the bias space (using `define_bias_space`) and then investigate whether to use `group_bias` or `mean_individual_bias`, which is not clear unless you have visited the basics of their standardization.\n",
    "\n",
    "\n",
    "Finally, the standardization implemented by wefe to execute the metrics allows run_query to execute routines that customize the execution of the metrics, such as word preprocessing, embeddings normalization and the calculation of submetrics or statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences between IJCAI version and Current version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most noticeable change we can mention with respect to the IJCAI version and the current version is the full implementation of a new debiasing methods module. It includes 5 methods of debiasing: `HardDebias`, `MulticlassHardDebias`, `DoubleHardDebias`, `RepulsionAttractionNeutralization` and `HalfSiblingRegression`.\n",
    "\n",
    "Regarding metrics: The original version of WEFE published in IJCAI contained 4 metrics: `WEAT`, `WEAT-ES`, `RND` and `RNSB`.\n",
    "Currently and thanks to contributions, WEFE also implements `MAC`, `RIPA` and `ECT`.\n",
    "\n",
    "Also, the original version contained very rudimentary `Query` and `WordEmbeddingModel` wrapper routines.\n",
    "\n",
    "In the actual version, the wrappers are much more complete and allow better interaction  with the user and with WEFE's internal APIs.  \n",
    "For example, the implementation of `__repr__` for Query and `WordEmbeddingModel` that show brief descriptions of each object to the user, as well as the implementation of `dict` method in query that allows to transform a query into a dictionary or update in `WordEmbeddingModel` that allows to update an embedding associated to a word for a new one.\n",
    "\n",
    "The `preprocessing` module was also improved: now it includes much more advanced logics (such as different preprocessing steps) which were modularized and generalized so that any metric or debias can use it.\n",
    "\n",
    "The documentation has been greatly enhanced compared to the original version by adding new user guides, as well as conceptual guides (where we explain the theorical framework), multi-language tutorials and detailed metrics and debias methods API documentation which also includes theoretical details.\n",
    "Finally, it is also worth mentioning that both testing and code quality was greatly improved from the original version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37d01894bb315c73bf6fde5551d8a97078996f38b23395695bd1998fb0ae5507"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
