{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark - WEFE, Fair Embedding Engine and Responsibly.AI\n",
    "\n",
    "To the best of our knowledge, we are aware of only three Python libraries that implement bias measurement and mitigation methods: Fair Embedding Engine (FEE) and Responsibly.\n",
    "\n",
    "According to its authors, Fair Embedding Engine () is defined as \"A Library for Analyzing and Mitigating Gender Bias in Word Embeddings\", while Responsibly () is defined as \"Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems.\"\n",
    "\n",
    "The FEE and Responsibly documentation can be found at the following links respectively: \n",
    "- https://github.com/FEE-Fair-Embedding-Engine/FEE\n",
    "- https://docs.responsibly.ai/\n",
    "\n",
    "The following document shows a comparison in various areas between these libraries with respect to WEFE.\n",
    "\n",
    "The points to be evaluated are:\n",
    "    \n",
    "1. Ease of installation\n",
    "2. Quality of the package and documentation.\n",
    "3. Ease of loading models\n",
    "4. Ease of running bias measurements. \n",
    "5. Performance in execution times.   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Metrics Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ease of installation\n",
    "\n",
    "This comparison aims to evaluate how easy it is to install the library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "According to the documentation, WEFE is available for installation using the Python Package Index (via pip) as well as via conda.\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install --upgrade wefe\n",
    "# or\n",
    "conda install -c pbadilla wefe\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair Embedding Engine\n",
    "\n",
    "In the case of FEE, neither the documentation nor the repository indicates how to install the package. Therefore, the easiest thing to do in this case is to clone the repository and then install the requirements manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clone the repo\n",
    "```bash\n",
    "$ git clone https://github.com/FEE-Fair-Embedding-Engine/FEE\n",
    "```\n",
    "\n",
    "2. Install the requirements.\n",
    "```bash\n",
    "$ pip install -r FEE/requirements.txt\n",
    "$ pip install sympy\n",
    "$ pip install -U gensim==3.8.3\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly\n",
    "\n",
    "According to its documentation, responsibly is also hosted in the Python Package Index so it can be installed using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pip install responsibly\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Both WEFE and responsibly are easy to install, which lowers the initial barriers to entry. FEE, on the other hand, requires more knowledge of Python to be able to use it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quality of the package and documentation\n",
    "\n",
    "This benchmark seeks to compare the quality of the documentation as well as the quality and best practices of the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "WEFE has a complete documentation page, which explains in detail the use of the package: an about with the motivation and objectives of the project, quick start showing how to install the library, multiple user manuals to measure and mitigate bias, detailed API of the implemented methods, theoretical manuals and finally implementations of previous case studies.\n",
    "\n",
    "In addition, most of the code is tested and was developed using continuous integration mechanisms (through a linter and testing in Github Actions) that keep it with a good code quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair Embedding Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEE has a documentation, which covers only the basic aspects of the API plus a flowchart showing the main concepts of the library.\n",
    "The documentation does not contain user guides, code examples or theoretical information about the implemented methods.\n",
    "\n",
    "On the other hand, no tests, linters or continuous code integration mechanisms could be identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responsibly has also a complete documentation page, which explains the use of the package: an index with the main project information and a quick start showing how to install the library, demos that act as user manuals, and a detailed API of the implemented methods.\n",
    "\n",
    "In addition, most of the code is tested and was developed using continuous integration mechanisms (through a linter and testing in Github Actions) that keep it with a good code quality.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In terms of documentation, WEFE contains much more detailed documentation than the other libraries with more extensive manuals and replications of previous case studies. \n",
    "Responsibly has sufficient documentation to execute its main functionalities without major problems, however, it is not exhaustive.\n",
    "FEE, on the other hand, only has API documentation, which makes it insufficient for new users to use it directly.\n",
    "\n",
    "With respect to software quality, both WEFE and Responsibly comply with best practices. \n",
    "FEE contains neither testing nor mechanisms to control code quality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ease of loading models\n",
    "\n",
    "This comparison looks at how easy it is to load a Word Embedding model.\n",
    "In this benchmark, two tests will be compared: loading a model from gensim API (`glove-twitter-25`) and loading a model from a binary file (`word2vec`).\n",
    "\n",
    "For the second test, you need to download the original word2vec model, which can be downloaded using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/word2vec-google-news-300.gz\n",
    "!gzip -dv word2vec-google-news-300.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "In WEFE, models are simply wrappers of Gensim models. This implies that the model reading process (either loaded by the API or by a file) is handled by the Gensim loaders, while the class that generates the objects that allow access to the embeddings is managed by WEFE.\n",
    "\n",
    "The following code can be used for the loading of the glove model from the gensim API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "import gensim.downloader as api\n",
    "\n",
    "# load glove\n",
    "twitter_25 = api.load('glove-twitter-25')\n",
    "model = WordEmbeddingModel(twitter_25, 'glove twitter dim=25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows you to load word2vec from its original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# load word2vec\n",
    "word2vec = KeyedVectors.load_word2vec_format('word2vec-google-news-300', binary=True)\n",
    "model = WordEmbeddingModel(word2vec, 'word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEE\n",
    "\n",
    "FEE also offers direct support for loading models from the FEE API through the following code.\n",
    "In this case, the model loading is coupled to the class which then has the methods to access the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FEE.fee.embedding.loader import WE\n",
    "\n",
    "fee_model = WE().load(ename = 'glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from FEE.fee.embedding.loader import WE\n",
    "\n",
    "fee_model = WE().load(fname = 'word2vec-google-news-300', format='bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly\n",
    "\n",
    "Responsibly has no intermediate interfaces to handle embedding models, it simply uses the gensim interface for this purpose. This can be reflected into the following script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_25 = api.load('glove-twitter-25')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('word2vec-google-news-300', binary=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the three libraries show similar behaviors and capabilities, which does not allow us to distinguish significant differences between them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ease of running bias measurements. \n",
    "\n",
    "This benchmark is intended to show how easy it is to run queries on the metrics that can be used. \n",
    "To keep the comparison simple, the set of words and the embeddings model will be kept fixed; only the metrics executed will be varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words to evaluate\n",
    "\n",
    "female_terms = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
    "male_terms = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "\n",
    "family_terms = [\n",
    "    \"home\",\n",
    "    \"parents\",\n",
    "    \"children\",\n",
    "    \"family\",\n",
    "    \"cousins\",\n",
    "    \"marriage\",\n",
    "    \"wedding\",\n",
    "    \"relatives\",\n",
    "]\n",
    "career_terms = [\n",
    "    \"executive\",\n",
    "    \"management\",\n",
    "    \"professional\",\n",
    "    \"corporation\",\n",
    "    \"salary\",\n",
    "    \"office\",\n",
    "    \"business\",\n",
    "    \"career\",\n",
    "]\n",
    "\n",
    "# optional, only for wefe use.\n",
    "target_sets_names = [\"Female Terms\", \"Male Terms\"]\n",
    "attribute_sets_names = [\"Arts\", \"Science\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEFE\n",
    "\n",
    "WEFE defines a standardized framework to execute metrics: in short, it is necessary to define a query that will act as a container for the words to be tested and then, together with the model, be delivered as input to some metric.\n",
    "\n",
    "The outputs of the metrics are always dictionaries since most of them contain additional information that could eventually be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Query: Female Terms and Male Terms wrt Arts and Science\n",
       "- Target sets: [['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter'], ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']]\n",
       "- Attribute sets:[['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives'], ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the modules\n",
    "from wefe.query import Query\n",
    "\n",
    "# 1. create the query\n",
    "query = Query(\n",
    "    [female_terms, male_terms],\n",
    "    [family_terms, career_terms],\n",
    "    target_sets_names,\n",
    "    attribute_sets_names,\n",
    ")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       " 'result': 0.31658412935212255,\n",
       " 'weat': 0.31658412935212255,\n",
       " 'effect_size': 0.6779439085309583,\n",
       " 'p_value': nan}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.metrics.WEAT import WEAT\n",
    "\n",
    "# 2. instance a WEAT metric and pass the query plus the model.\n",
    "weat = WEAT()\n",
    "result = weat.run_query(query, model)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As run query is independent of the query and the model, it can take several parameters that customize the performance of the metric. In this case, we show how to standardize the words before searching for them in the model by making them all lowercase and then removing their accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       " 'result': 0.31658412935212255,\n",
       " 'weat': 0.31658412935212255,\n",
       " 'effect_size': 0.6779439085309583,\n",
       " 'p_value': nan}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weat = WEAT()\n",
    "result = weat.run_query(\n",
    "    query,\n",
    "    model,\n",
    "    preprocessors=[{\"lowercase\": True, \"strip_accents\": True}],\n",
    ")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we show how to calculate the p-value through a permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       " 'result': 0.31658412935212255,\n",
       " 'weat': 0.31658412935212255,\n",
       " 'effect_size': 0.6779439085309583,\n",
       " 'p_value': 0.0888911108889111}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weat = WEAT()\n",
    "result = weat.run_query(\n",
    "    query,\n",
    "    model,\n",
    "    calculate_p_value=True,\n",
    ")\n",
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interface makes it possible for us to switch very easily to similar metrics (i.e. supporting the same number of word sets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       " 'result': 0.216306751034807,\n",
       " 'rnsb': 0.216306751034807,\n",
       " 'negative_sentiment_probabilities': {'female': 0.16418179646736974,\n",
       "  'woman': 0.10820153944001121,\n",
       "  'girl': 0.020617838110230324,\n",
       "  'sister': 0.027141541058957608,\n",
       "  'she': 0.02994207142697125,\n",
       "  'her': 0.01769030389841153,\n",
       "  'hers': 0.19920453536673288,\n",
       "  'daughter': 0.030796399892449533,\n",
       "  'male': 0.06336197706763091,\n",
       "  'man': 0.0960030254620704,\n",
       "  'boy': 0.056068043904550446,\n",
       "  'brother': 0.0670556581455255,\n",
       "  'he': 0.11074181520707649,\n",
       "  'him': 0.04994848590137757,\n",
       "  'his': 0.11684191035273095,\n",
       "  'son': 0.1483104749013432},\n",
       " 'negative_sentiment_distribution': {'female': 0.1257031346581953,\n",
       "  'woman': 0.08284275708455253,\n",
       "  'girl': 0.015785713983500267,\n",
       "  'sister': 0.020780481539213497,\n",
       "  'she': 0.02292466227994957,\n",
       "  'her': 0.013544294805717852,\n",
       "  'hers': 0.15251772774154254,\n",
       "  'daughter': 0.023578765039506653,\n",
       "  'male': 0.04851207202574892,\n",
       "  'man': 0.07350316232927329,\n",
       "  'boy': 0.04292759017505359,\n",
       "  'brother': 0.05134007914900688,\n",
       "  'he': 0.0847876781031249,\n",
       "  'him': 0.03824224965452664,\n",
       "  'his': 0.08945811720186142,\n",
       "  'son': 0.11355151422922607}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.metrics import RNSB\n",
    "\n",
    "rnsb = RNSB()\n",
    "result = rnsb.run_query(query, model)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Terms and Male Terms wrt Arts and Science',\n",
       " 'result': 0.4357533518195851,\n",
       " 'mac': 0.4357533518195851,\n",
       " 'targets_eval': {'Female Terms': {'female': {'Arts': 0.31804752349853516,\n",
       "    'Science': 0.43660861998796463},\n",
       "   'woman': {'Arts': 0.24169018119573593, 'Science': 0.3958834111690521},\n",
       "   'girl': {'Arts': 0.27893901616334915, 'Science': 0.5540130585432053},\n",
       "   'sister': {'Arts': 0.26786402612924576, 'Science': 0.5402307193726301},\n",
       "   'she': {'Arts': 0.3126588687300682, 'Science': 0.5178160294890404},\n",
       "   'her': {'Arts': 0.31602276116609573, 'Science': 0.5942907352000475},\n",
       "   'hers': {'Arts': 0.4396950639784336, 'Science': 0.5640630088746548},\n",
       "   'daughter': {'Arts': 0.2509744167327881, 'Science': 0.5034244172275066}},\n",
       "  'Male Terms': {'male': {'Arts': 0.3604205995798111,\n",
       "    'Science': 0.5834408048540354},\n",
       "   'man': {'Arts': 0.3802716135978699, 'Science': 0.5215189531445503},\n",
       "   'boy': {'Arts': 0.32475025951862335, 'Science': 0.5474967788904905},\n",
       "   'brother': {'Arts': 0.281925693154335, 'Science': 0.48872119933366776},\n",
       "   'he': {'Arts': 0.400515902787447, 'Science': 0.5290917791426182},\n",
       "   'him': {'Arts': 0.29920244961977005, 'Science': 0.47434185072779655},\n",
       "   'his': {'Arts': 0.300054918974638, 'Science': 0.39712197706103325},\n",
       "   'son': {'Arts': 0.6768742799758911, 'Science': 0.846136340405792}}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wefe.metrics import MAC\n",
    "\n",
    "rnsb = MAC()\n",
    "result = rnsb.run_query(query, model)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fair Embedding Engine\n",
    "\n",
    "In the case of Fair Embedding Engine, the embedding model is delivered at the time of instantiating the metric and then through the compute method its value is calculated.\n",
    "\n",
    "In this case, FEE differs somewhat from WEFE normalization by making each instance of the metric model-dependent.\n",
    "\n",
    "On the other hand, we can see that the word sets are delivered directly as star * (arbitrary number of positional arguments) parameters, which makes it difficult to understand how many and which word sets to pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7264477"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from FEE.fee.metrics import WEAT as FEE_WEAT\n",
    "fee_weat = FEE_WEAT(fee_model)\n",
    "\n",
    "fee_weat.compute(female_terms, male_terms, family_terms, career_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEAT's implementation of FEE also allows the p_value to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7264477, 0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fee_weat.compute(female_terms, male_terms, family_terms, career_terms, p_val=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does not contain the possibility of executing more complex actions such as preprocessing word sets.\n",
    "\n",
    "Finally, we were not able to find any other metric that was easily replaceable using the same interface (unlike with WEFE and its standardization layer)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsibly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wefe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37d01894bb315c73bf6fde5551d8a97078996f38b23395695bd1998fb0ae5507"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
